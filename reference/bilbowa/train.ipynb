{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "from os.path import join\n",
    "import time\n",
    "\n",
    "from absl import app\n",
    "from absl import flags\n",
    "from absl import logging\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# from data import Embedding, MultiLanguageEmbedding, \\\n",
    "#     LazyIndexCorpus,  Word2vecIterator, BilbowaIterator\n",
    "\n",
    "from data import *\n",
    "from model import get_model, word2vec_loss, bilbowa_loss, strong_pair_loss, weak_pair_loss\n",
    "import sys\n",
    "sys.path.insert(0, '../../reference/eval')\n",
    "from evaluate import Evaluator\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 995003/995003 [00:17<00:00, 56611.95 words/s]\n",
      "100%|██████████| 432455/432455 [00:04<00:00, 88011.86 words/s] \n",
      " 77%|███████▋  | 763484/995003 [00:13<00:04, 54872.23 words/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-f40426c6a8d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0memb_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0memb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_emb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mctxemb0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./data_root/withctx.en-fr.en.50.1.txt.ctx\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mctxemb1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./data_root/withctx.en-fr.fr.50.1.txt.ctx\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mctxemb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultiLanguageEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctxemb0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctxemb1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/Setapp/GD/research/cross-lingual/bilingual_dict_embeddings/reference/bilbowa/data.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, emb_file, stopwords_file, keep_emb)\u001b[0m\n\u001b[1;32m     33\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocablower2id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_emb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m                 emb_file, keep_emb=keep_emb)\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstopwords_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/Setapp/GD/research/cross-lingual/bilingual_dict_embeddings/reference/bilbowa/data.py\u001b[0m in \u001b[0;36mload_emb\u001b[0;34m(self, filepath, keep_emb)\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m' words'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m                 \u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ],
     "output_type": "error"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 77%|███████▋  | 763484/995003 [00:30<00:09, 25443.79 words/s]"
     ]
    }
   ],
   "source": [
    "emb0 = Embedding(\"./data_root/withctx.en-fr.en.50.1.txt\")\n",
    "emb1 = Embedding(\"./data_root/withctx.en-fr.fr.50.1.txt\")\n",
    "emb = MultiLanguageEmbedding(emb0, emb1)\n",
    "vocab = emb.get_vocab()\n",
    "emb_matrix = emb.get_emb()\n",
    "\n",
    "ctxemb0 = Embedding(\"./data_root/withctx.en-fr.en.50.1.txt.ctx\")\n",
    "ctxemb1 = Embedding(\"./data_root/withctx.en-fr.fr.50.1.txt.ctx\")\n",
    "ctxemb = MultiLanguageEmbedding(ctxemb0, ctxemb1)\n",
    "ctxvocab = ctxemb.get_vocab()\n",
    "ctxemb_matrix = ctxemb.get_emb()\n",
    "\n",
    "evaluator = Evaluator(emb0, emb1) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "strong, weak = read_pair()\n",
    "strong_id, weak_id, l0_dict, l1_dict = pair2id(strong, weak, emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert tuple(ctxvocab) == tuple(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "mono_max_lines = 10000\n",
    "mono0 = LazyIndexCorpus(\"./data_root/en_mono\",\n",
    "        max_lines=mono_max_lines)\n",
    "\n",
    "mono1 = LazyIndexCorpus(\"./data_root/en_mono\",mono_max_lines)\n",
    "\n",
    "multi_max_lines = 10000\n",
    "multi0 = LazyIndexCorpus(\"./data_root/en_multi\",max_lines=multi_max_lines)\n",
    "multi1 = LazyIndexCorpus(\"./data_root/fr_multi\",max_lines=multi_max_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "mono0_unigram_table = mono0.get_unigram_table(vocab_size=len(vocab))\n",
    "mono1_unigram_table = mono1.get_unigram_table(vocab_size=len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_subsample = 1e-5\n",
    "word2vec_negative_size = 10\n",
    "word2vec_batch_size = 100000\n",
    "mono0_iterator = Word2vecIterator(\n",
    "        mono0,\n",
    "        mono0_unigram_table,\n",
    "        subsample=emb_subsample,\n",
    "        window_size=word2vec_negative_size,\n",
    "        negative_samples=word2vec_negative_size,\n",
    "        batch_size=word2vec_batch_size,\n",
    "    )\n",
    "\n",
    "mono1_iterator = Word2vecIterator(\n",
    "        mono1,\n",
    "        mono1_unigram_table,\n",
    "        subsample=emb_subsample,\n",
    "        window_size=word2vec_negative_size,\n",
    "        negative_samples=word2vec_negative_size,\n",
    "        batch_size=word2vec_batch_size,\n",
    "    )\n",
    "\n",
    "bilbowa_sent_length = 50\n",
    "bilbowa_batch_size = 100\n",
    "multi_iterator = BilbowaIterator(\n",
    "    multi0,\n",
    "    multi1,\n",
    "    mono0_unigram_table,\n",
    "    mono1_unigram_table,\n",
    "    subsample=emb_subsample,\n",
    "    length=bilbowa_sent_length,\n",
    "    batch_size=bilbowa_batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# strong pair iterator\n",
    "strong_batch_size = 1000\n",
    "strong_negative_size = 10\n",
    "strong_pair_iterator = strong_pairIterator(\n",
    "    strong_id,\n",
    "    mono0_unigram_table,\n",
    "    mono1_unigram_table,\n",
    "    batch_size = strong_batch_size,\n",
    "    negative_samples = strong_negative_size,\n",
    "    l0_dict = l0_dict,\n",
    "    l1_dict = l1_dict\n",
    ")\n",
    "\n",
    "# weak pair iterator\n",
    "weak_batch_size = 3000\n",
    "weak_negative_size = 10\n",
    "weak_pair_iterator = weak_pairIterator(\n",
    "    weak_id,\n",
    "    mono0_unigram_table,\n",
    "    mono1_unigram_table,\n",
    "    batch_size=weak_batch_size,\n",
    "    negative_samples=weak_negative_size,\n",
    "    l0_dict=l0_dict,\n",
    "    l1_dict=l1_dict\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUTPUT Tensor(\"dot_2/MatMul:0\", shape=(?, 1, 1), dtype=float32)\n",
      "OUTPUT Tensor(\"flatten_3/Reshape:0\", shape=(?, ?), dtype=float32)\n",
      "OUTPUT Tensor(\"multiply_1/mul:0\", shape=(?, ?), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "emb_dim = 50\n",
    "encoder_desc_length = 15\n",
    "(\n",
    "    word2vec_model,\n",
    "    bilbowa_model,\n",
    "    strong_pair_model,\n",
    "    weak_pair_model,\n",
    "    word2vec_model_infer,\n",
    "    bilbowa_model_infer,\n",
    "    strong_pair_model_infer,\n",
    "    weak_pair_model_infer\n",
    ") = get_model(\n",
    "    nb_word=len(vocab),\n",
    "    dim=emb_dim,\n",
    "    length=bilbowa_sent_length,\n",
    "    desc_length=encoder_desc_length,\n",
    "    word_emb_matrix=emb_matrix,\n",
    "    context_emb_matrix=ctxemb_matrix,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info('word2vec_model.summary()')\n",
    "word2vec_model.summary()\n",
    "logging.info('bilbowa_model.summary()')\n",
    "bilbowa_model.summary()\n",
    "logging.info('strong_pair_model.summary()')\n",
    "strong_pair_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_lr = 0.001\n",
    "word2vec_model.compile(\n",
    "    optimizer=(Adam(amsgrad=True) if word2vec_lr < 0 else Adam(\n",
    "        lr=word2vec_lr, amsgraword2vec_modeld=True)),\n",
    "    loss=word2vec_loss)\n",
    "\n",
    "bilbowa_lr = 0.001\n",
    "bilbowa_model.compile(\n",
    "    optimizer=(Adam(amsgrad=True) if bilbowa_lr < 0 else Adam(\n",
    "        lr=bilbowa_lr, amsgrad=True)),\n",
    "    loss=bilbowa_loss)\n",
    "\n",
    "strong_pair_model_lr = 0.001\n",
    "strong_pair_model.compile(\n",
    "    optimizer=(Adam(amsgrad=True) if strong_pair_model_lr < 0 else Adam(\n",
    "        lr=strong_pair_model_lr, amsgrad=True)),\n",
    "    loss=strong_pair_loss)\n",
    "\n",
    "weak_pair_model_lr = 0.001\n",
    "weak_pair_model.compile(\n",
    "    optimizer=(Adam(amsgrad=True) if weak_pair_model_lr < 0 else Adam(\n",
    "        lr=weak_pair_model_lr, amsgrad=True)),\n",
    "    loss=weak_pair_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mono0_iter = mono0_iterator.fast2_iter()\n",
    "mono1_iter = mono1_iterator.fast2_iter()\n",
    "multi_iter = multi_iterator.iter()\n",
    "strong_iter = strong_pair_iterator.strong_iter()\n",
    "weak_iter = weak_pair_iterator.weak_iter()\n",
    "\n",
    "keys = []\n",
    "keys.append('mono0')\n",
    "keys.append('mono1')\n",
    "keys.append('multi')\n",
    "keys.append('strong_pair')\n",
    "keys.append('weak_pair')\n",
    "keys = tuple(keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_to_str(d):\n",
    "    return '{' + ', '.join(\n",
    "        ['%s: %s' % (key, d[key]) for key in sorted(d.keys())]) + '}'\n",
    "\n",
    "comp_time = {key: 0.0 for key in keys}\n",
    "load_time = {key: 0.0 for key in keys}\n",
    "hit_count = {key: 0 for key in keys}\n",
    "iter_info = {key: (0, 0) for key in keys}\n",
    "last_loss = {key: 0.0 for key in keys}\n",
    "\n",
    "def get_total_time():\n",
    "    return {key: comp_time[key] + load_time[key] for key in keys}\n",
    "\n",
    "global_start_time = time.time()\n",
    "last_logging_time = 0.\n",
    "loss_decay = 0.6\n",
    "last_saving_time = 0.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    total_time = get_total_time()\n",
    "    target_time = total_time\n",
    "    min_time = min(target_time.values())\n",
    "    next_key = [key for key in keys if target_time[key] == min_time][0]\n",
    "    \n",
    "    if next_key == 'mono0':\n",
    "        start_time = time.time()\n",
    "        (x, y), (epoch, instance) = next(mono0_iter)\n",
    "        this_load_time = time.time() - start_time\n",
    "        start_time = time.time()\n",
    "        loss = word2vec_model.train_on_batch(x=x, y=y)\n",
    "        this_comp_time = time.time() - start_time\n",
    "    elif next_key == 'mono1':\n",
    "        start_time = time.time()\n",
    "        (x, y), (epoch, instance) = next(mono1_iter)\n",
    "        this_load_time = time.time() - start_time\n",
    "        start_time = time.time()\n",
    "        loss = word2vec_model.train_on_batch(x=x, y=y)\n",
    "        this_comp_time = time.time() - start_time\n",
    "    elif next_key == 'multi':\n",
    "        start_time = time.time()\n",
    "        (x, y), (epoch, instance) = next(multi_iter)\n",
    "        this_load_time = time.time() - start_time\n",
    "        start_time = time.time()\n",
    "        loss = bilbowa_model.train_on_batch(x=x, y=y)\n",
    "        this_comp_time = time.time() - start_time\n",
    "    elif next_key == 'strong_pair':\n",
    "        start_time = time.time()\n",
    "        (x, y), (epoch, instance) = next(strong_iter)\n",
    "        this_load_time = time.time() - start_time\n",
    "        start_time = time.time()\n",
    "        loss = strong_pair_model.train_on_batch(x=x, y=y)\n",
    "        this_comp_time = time.time() - start_time\n",
    "    elif next_key == 'weak_pair':\n",
    "        start_time = time.time()\n",
    "        (x, y), (epoch, instance) = next(weak_iter)\n",
    "        this_load_time = time.time() - start_time\n",
    "        start_time = time.time()\n",
    "        loss = weak_pair_model.train_on_batch(x=x, y=y)\n",
    "        this_comp_time = time.time() - start_time\n",
    "    else:\n",
    "        assert False\n",
    "\n",
    "#     assert not math.isnan(loss)\n",
    "\n",
    "    comp_time[next_key] += this_comp_time\n",
    "    load_time[next_key] += this_load_time\n",
    "    hit_count[next_key] += 1\n",
    "    iter_info[next_key] = (epoch, instance)\n",
    "    last_loss[next_key] = loss if last_loss[next_key] == 0.0 else (\n",
    "        last_loss[next_key] * loss_decay + loss * (1. - loss_decay))\n",
    "\n",
    "    # exit if target is reached\n",
    "    should_exit = False\n",
    "    if FLAGS.max_mono_epochs > -1:\n",
    "        if (iter_info['mono0'][0] >= FLAGS.max_mono_epochs\n",
    "                and iter_info['mono1'][0] >= FLAGS.max_mono_epochs):\n",
    "            should_exit = True\n",
    "\n",
    "    if FLAGS.max_multi_epochs > -1:\n",
    "        if (iter_info['multi'][0] >= FLAGS.max_multi_epochs):\n",
    "            should_exit = True\n",
    "\n",
    "    total_this_comp_time = time.time() - global_start_time\n",
    "    if should_exit or (total_this_comp_time - last_logging_time >\n",
    "                       FLAGS.logging_iterval):\n",
    "        last_logging_time = total_this_comp_time\n",
    "        # logging.info('Stats so far')\n",
    "        # logging.info('next_key = %s', next_key)\n",
    "        # logging.info('comp_time = %s', dict_to_str(comp_time))\n",
    "        # logging.info('load_time = %s', dict_to_str(load_time))\n",
    "        # logging.info('total_time = %s', dict_to_str(get_total_time()))\n",
    "        logging.info('hit_count = %s', dict_to_str(hit_count))\n",
    "        # logging.info('iter_info = %s', dict_to_str(iter_info))\n",
    "        logging.info('last_loss = %s', dict_to_str(last_loss))\n",
    "        \n",
    "        #evaluate:\n",
    "        evaluator.word_translation\n",
    "\n",
    "    # save model\n",
    "    if should_exit or (total_this_comp_time - last_saving_time >\n",
    "                       FLAGS.saving_iterval):\n",
    "        last_saving_time = total_this_comp_time\n",
    "        logging.info('Saving models started.')\n",
    "        tag = ''\n",
    "        word2vec_model.save(join(FLAGS.model_root, tag + 'word2vec_model'))\n",
    "        bilbowa_model.save(join(FLAGS.model_root, tag + 'bilbowa_model'))\n",
    "        word2vec_model_infer.save(\n",
    "            join(FLAGS.model_root, tag + 'word2vec_model_infer'))\n",
    "        bilbowa_model_infer.save(\n",
    "            join(FLAGS.model_root, tag + 'bilbowa_model_infer'))\n",
    "        logging.info('Saving models done.')\n",
    "\n",
    "    if should_exit:\n",
    "        logging.info('Training target reached. Exit.')\n",
    "        break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
